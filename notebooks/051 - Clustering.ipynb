{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Description\" data-toc-modified-id=\"Description-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Description</a></span></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setup</a></span></li><li><span><a href=\"#Other-functions\" data-toc-modified-id=\"Other-functions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Other functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Filter\" data-toc-modified-id=\"Filter-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Filter</a></span></li><li><span><a href=\"#Clustering\" data-toc-modified-id=\"Clustering-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Clustering</a></span></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Evaluation</a></span></li><li><span><a href=\"#Plotting\" data-toc-modified-id=\"Plotting-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Plotting</a></span></li><li><span><a href=\"#Grid-Wrapper\" data-toc-modified-id=\"Grid-Wrapper-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Grid Wrapper</a></span></li></ul></li><li><span><a href=\"#Clustering\" data-toc-modified-id=\"Clustering-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Clustering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Options\" data-toc-modified-id=\"Options-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Options</a></span></li></ul></li><li><span><a href=\"#Visualize\" data-toc-modified-id=\"Visualize-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Visualize</a></span><ul class=\"toc-item\"><li><span><a href=\"#Graph\" data-toc-modified-id=\"Graph-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Graph</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cluster funds using Spektral Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import feather\n",
    "from itertools import product\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import sparse\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "from minisom import MiniSom\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from sklearn.preprocessing import normalize, LabelEncoder\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score, davies_bouldin_score\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "from graph_tool.all import *\n",
    "import cairo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Returns\n",
    "path = '../data/processed/returns.feather'\n",
    "returns = feather.read_dataframe(path)\n",
    "returns = returns.rename(columns = {'caldt' : 'report_dt'})\n",
    "returns = returns.assign(year = returns['report_dt'].dt.year)\n",
    "\n",
    "### row_info\n",
    "path = '../data/processed/row_info.feather'\n",
    "row_info = feather.read_dataframe(path)\n",
    "le = LabelEncoder()\n",
    "row_info['lipper_class_num'] = le.fit_transform(row_info['lipper_class'])\n",
    "\n",
    "### col_info\n",
    "path = '../data/processed/col_info.feather'\n",
    "col_info = feather.read_dataframe(path)\n",
    "\n",
    "### Holdings\n",
    "path = '../data/processed/holdings.npz'\n",
    "holdings = sparse.load_npz(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(param, verbose = False):\n",
    "    \n",
    "    verbose = param['verbose']\n",
    "    year = param['year']\n",
    "        \n",
    "    row_info_f = row_info.copy()\n",
    "    if (year != 'full'):    # If year = full take whole sample\n",
    "        row_info_f = row_info_f.query('year == @year')\n",
    "\n",
    "    begin_date = row_info_f.iloc[0,:]['report_dt']\n",
    "    end_date = begin_date + pd.DateOffset(years=1) # 1 year offset\n",
    "    row_info_f.reset_index(drop = True, inplace=True)\n",
    "\n",
    "    # Filter returns\n",
    "    crsp_fundno_unique = row_info_f['crsp_fundno'].unique()\n",
    "    returns_f = returns.copy()\n",
    "    query = '''report_dt >= @begin_date and report_dt <= @end_date and crsp_fundno in @crsp_fundno_unique'''\n",
    "    returns_f = returns_f.query(query)\n",
    "\n",
    "    # Change return of month for which holdings apply to 0\n",
    "    returns_f = returns_f.copy()\n",
    "    mask = returns_f['report_dt'] == begin_date\n",
    "    returns_f.loc[mask,'mret'] = 0\n",
    "    \n",
    "    # Drop all funds with first return observation after starting date\n",
    "    drop_fundnos = returns_f.drop_duplicates('crsp_fundno').query('mret != 0')['crsp_fundno']\n",
    "    returns_f.query('crsp_fundno not in @drop_fundnos', inplace=True)\n",
    "    row_info_f.query('crsp_fundno not in @drop_fundnos', inplace=True)\n",
    "    \n",
    "    # Filter holdings accordingly and delet all securities with less than two observations\n",
    "    holdings_f = holdings.copy()\n",
    "    holdings_f = holdings[row_info_f['row']]\n",
    "    \n",
    "    holdings_b = sparse.csr_matrix(holdings_f, copy=True)\n",
    "    holdings_b.data = np.ones(len(holdings_f.data))\n",
    "\n",
    "    sum_sec_boolean = holdings_b.toarray().sum(0)\n",
    "    col_mask = (sum_sec_boolean >= 2).flatten()\n",
    "\n",
    "    holdings_f = holdings_f.tocsc()\n",
    "    holdings_f = holdings_f[:,col_mask]\n",
    "    holdings_f = holdings_f.tocsr()\n",
    "    \n",
    "    ## Preprocessing\n",
    "    preprocessing = param['preprocessing']\n",
    "    if (preprocessing == 'none'): holdings_ft = holdings_f\n",
    "    if (preprocessing == 'l1'):   holdings_ft = normalize(holdings_f, norm = 'l1')\n",
    "    if (preprocessing == 'l2'):   holdings_ft = normalize(holdings_f, norm = 'l2')\n",
    "\n",
    "    \n",
    "    if (verbose):\n",
    "        print('Numer of unique funds:           {:10,d}'.format(row_info_f.shape[0]))\n",
    "\n",
    "        print('Numer of unique securities:      {:10,d}'.format(holdings_ft.shape[1]))\n",
    "\n",
    "        print('Begin date:                      {}'.format(begin_date.date()))\n",
    "        print('End date:                        {}'.format(end_date.date()))\n",
    "    \n",
    "    return(row_info_f, returns_f, holdings_ft, begin_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_clustering(holdings_ft, param):\n",
    "    verbose = param['verbose']\n",
    "\n",
    "    if(verbose): print('Start clustering...')\n",
    "    clustering = SpectralClustering(n_clusters = param['n_clusters'],\n",
    "                                    assign_labels = param['assign_labels'], # kmeans or discretize\n",
    "                                    eigen_solver = 'arpack',\n",
    "                                    affinity = param['affinity'],\n",
    "                                    gamma = param['gamma'],\n",
    "                                    n_init = param['n_init'],\n",
    "                                    n_jobs = -1,\n",
    "                                    random_state = 0).fit(holdings_ft)\n",
    "    if(verbose): print('Clustering finished')\n",
    "    \n",
    "    return(clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_clustering(holdings_ft, param):\n",
    "    verbose = param['verbose']\n",
    "\n",
    "    if(verbose): print('Start clustering...')\n",
    "    clustering = KMeans(n_clusters = param['n_clusters'],\n",
    "                        verbose = verbose,\n",
    "                        n_init = param['n_init'], # Number of runs\n",
    "                        n_jobs= -1,\n",
    "                        random_state = 1\n",
    "                       ).fit(holdings_ft)\n",
    "    \n",
    "    if(verbose): print('Clustering finished')\n",
    "    \n",
    "    return(clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def som_clustering(holdings_ft, param):\n",
    "    verbose = param['verbose']\n",
    "    if(verbose): print('Start clustering...')\n",
    "    \n",
    "    ### Initialization and training ###\n",
    "    # Configure SOM\n",
    "    som = MiniSom(x = 25,\n",
    "                  y = 25,\n",
    "                  input_len = holdings_ft.shape[1],\n",
    "    #             neighborhood_function = 'triangle',\n",
    "                  sigma = 2.0,\n",
    "                  learning_rate = 0.5)\n",
    "\n",
    "    # Initialize\n",
    "    data = holdings_ft.toarray()\n",
    "    som.random_weights_init(data)\n",
    "\n",
    "    # Train\n",
    "    som.train_random(data, param['training_epochs'], verbose = verbose) # training with 100 iterations\n",
    "    \n",
    "    som_quantized = som.quantization(data)\n",
    "\n",
    "    clustering = KMeans(n_clusters = param['n_clusters'],\n",
    "                        verbose = verbose,\n",
    "                        n_init = param['n_init'], # Number of runs\n",
    "                        n_jobs= -1,\n",
    "                        random_state = 1\n",
    "                       ).fit(som_quantized)\n",
    "\n",
    "    if(verbose): print('Clustering finished')    \n",
    "    return(clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_styleadj_returns(row_info_f, returns_f, style_cols):\n",
    "\n",
    "    row_info_m = row_info_f.copy()\n",
    "    returns_m = returns_f.copy()\n",
    "    \n",
    "    returns_m = returns_m.sort_values(['crsp_fundno','report_dt'])\n",
    "\n",
    "    # merge predicted styles onto returns\n",
    "    returns_m = returns_m.merge(row_info_m[['crsp_fundno', 'report_dt', style_cols]],\n",
    "                            how='left',\n",
    "                            on=['crsp_fundno', 'report_dt'])\n",
    "\n",
    "    # Forward fill all styles and drop nas\n",
    "    returns_m = (returns_m.apply(lambda x: x.fillna(method = 'ffill'))\n",
    "    )\n",
    "\n",
    "    # Calc mean return per style\n",
    "    style_returns = (returns_m\n",
    "                        .groupby([style_cols,'report_dt'])\n",
    "                        .mean()\n",
    "                        .reset_index()\n",
    "                        .drop(columns='crsp_fundno')\n",
    "    )\n",
    "\n",
    "    # Merge style returns onto fund returns and calc tracking error\n",
    "    returns_m = (returns_m\n",
    "                        .rename(columns = {'mret' : 'fund_ret'}) \n",
    "                        .merge(style_returns,\n",
    "                                    how = 'left',\n",
    "                                    on = [style_cols,'report_dt'])\n",
    "                        .assign(error = lambda df: df['fund_ret'] - df['mret'])\n",
    "                        .rename(columns = {'mret' : 'style_ret'}) \n",
    "    )\n",
    "\n",
    "    returns_m = returns_m[['crsp_fundno', 'report_dt', style_cols,\n",
    "                       'fund_ret', 'style_ret', 'error']]\n",
    "\n",
    "    return(returns_m, style_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_vola_deciles(returns_m): \n",
    "    error_vol = (returns_m\n",
    "                 .groupby(['crsp_fundno'])['error']\n",
    "                 .std()\n",
    "                 .reset_index())\n",
    "    error_vol['error'] = error_vol['error'] * 100\n",
    "    \n",
    "    error_vol = (error_vol[['error']]\n",
    "                .apply(lambda x : x.quantile(np.round(np.arange(0.1,1,0.1),2))))\n",
    "    return(error_vol)\n",
    "\n",
    "def error_vola_describe(returns_m): \n",
    "    error_vol = (returns_m\n",
    "                 .groupby(['crsp_fundno'])['error']\n",
    "                 .std()\n",
    "                 .reset_index())\n",
    "    error_vol['error'] = error_vol['error'] * 100\n",
    "    return(error_vol[['error']].describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation(row_info_f, returns_f, n_iterations):\n",
    "    np.random.seed()\n",
    "\n",
    "    #n_iterations = 500\n",
    "    \n",
    "    # First choose n samples of funds with one fund per cluster\n",
    "    funds_list = []\n",
    "    cluster = np.array(row_info_f[['crsp_fundno','cluster']])\n",
    "    arr = np.arange(row_info_f.shape[0])\n",
    "\n",
    "    for i in np.arange(n_iterations):\n",
    "        np.random.shuffle(arr)\n",
    "        cluster = cluster[arr]\n",
    "        index = np.unique(cluster[:,1], return_index = True, return_inverse = False)[1]\n",
    "        funds = cluster[index,0]\n",
    "        funds_list.append(funds)\n",
    "\n",
    "\n",
    "    mean_return = []\n",
    "    mean_std = []\n",
    "    returns_fundnos = returns_f['crsp_fundno'].values\n",
    "\n",
    "    for funds in funds_list:\n",
    "        # Take returns for sample and calc equally weighted average return\n",
    "        returns_index = np.isin(returns_fundnos,funds)\n",
    "        returns_s = returns_f[returns_index]\n",
    "        returns_s = returns_s.groupby('report_dt')['mret'].mean()\n",
    "\n",
    "        # Calc mean and std\n",
    "        mean_return.append(returns_s.std())\n",
    "        mean_std.append(returns_s.mean())\n",
    "\n",
    "\n",
    "    mean_return = pd.DataFrame(mean_return).mean()\n",
    "    mean_std = pd.DataFrame(mean_std).mean()\n",
    "    \n",
    "    return([mean_return[0], mean_std[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation_wrapper(row_info_f, returns_f, n_iterations):\n",
    "    result_list = []\n",
    "    pool = Pool()\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        pool.apply_async(simulation, callback = result_list.append)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    result = result_list\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_map(row_info_f):\n",
    "    cap = [0,1,2,3]\n",
    "    style = [0,1,2]\n",
    "\n",
    "    counts = row_info_f['cluster'].value_counts().sort_index()\n",
    "    size = np.round(counts / 10)\n",
    "\n",
    "    data = round(\n",
    "            pd.crosstab(\n",
    "                row_info_f['cap_class'],row_info_f['cluster'], \n",
    "                margins = True, normalize = 'columns') * 100, 2).T\n",
    "\n",
    "    x = data.apply(lambda x: np.sum(x * cap) / 100, axis = 1)\n",
    "\n",
    "    data = round(\n",
    "            pd.crosstab(\n",
    "                row_info_f['style_class'],row_info_f['cluster'], \n",
    "                margins = True, normalize = 'columns') * 100, 2).T\n",
    "    y = data.apply(lambda x: np.sum(x * style) / 100, axis = 1)\n",
    "\n",
    "    label = x.index[:-1]\n",
    "\n",
    "    fig = plt.figure(figsize=(15,4))\n",
    "    ax_s = fig.add_subplot(111)\n",
    "\n",
    "    #ax_s.grid(True)\n",
    "\n",
    "    plt.xlabel('Market cap dimension')\n",
    "    plt.xticks([0,1,2,3], ['SC','MC','ML','LC'])\n",
    "\n",
    "    plt.ylabel('Style dimension')\n",
    "    plt.yticks([0,1,2], ['V','C','G'])\n",
    "\n",
    "    for i, txt in enumerate(label):\n",
    "        ax_s.annotate(txt, (x[i], y[i]),\n",
    "                     xytext = (0, 0),              # Horizontally shift label by `space`\n",
    "                     textcoords = 'offset points', # Interpret `xytext` as offset in points\n",
    "                     va='center',                  # Vertically center label\n",
    "                     ha='center',\n",
    "                     color = 'black',\n",
    "                     size = size[i])  \n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def plot_cluster(row_info_f, style, ax):\n",
    "    data = round(\n",
    "        pd.crosstab(\n",
    "            row_info_f[style],row_info_f['cluster'], \n",
    "            margins = True, normalize = 'columns') * 100, 2).T\n",
    "\n",
    "    data.plot(kind='bar', \n",
    "                 stacked=True, ax = ax)\n",
    "\n",
    "    ax.legend().remove()\n",
    "    label_list = data.columns.values.astype(str).repeat(data.shape[0])\n",
    "    rects = ax.patches\n",
    "\n",
    "    # For each bar: Place a label\n",
    "    for i, rect in enumerate(rects):\n",
    "        if rect.get_height() > 10:\n",
    "            # Get X and Y placement of label from rect.\n",
    "            x_value = rect.get_x() + rect.get_width() / 2\n",
    "            y_value = rect.get_y() + rect.get_height() / 2\n",
    "\n",
    "            # Use X value as label and format number with one decimal place\n",
    "            label = \"{}\".format(label_list[i])\n",
    "\n",
    "            # Create annotation\n",
    "            ax.annotate(\n",
    "                label,                        # Use `label` as label\n",
    "                (x_value, y_value),           # Place label at end of the bar\n",
    "                xytext = (0, 0),              # Horizontally shift label by `space`\n",
    "                textcoords = 'offset points', # Interpret `xytext` as offset in points\n",
    "                va='center',                  # Vertically center label\n",
    "                ha='center',\n",
    "                color = 'white',\n",
    "                size = 12)                  # Horizontally align label \n",
    "    return(ax)\n",
    "    \n",
    "def plot_cluster_wrapper(row_info_f):\n",
    "    \n",
    "    f, axes = plt.subplots(nrows = 4, ncols=1, sharex=True, \n",
    "                           figsize = (15,6), gridspec_kw={'height_ratios':[1,2,2,2]})\n",
    "    \n",
    "    data = row_info_f['cluster'].value_counts(sort=False).append(to_append = pd.Series([0]))\n",
    "    data.plot(kind='bar', ax = axes[0])\n",
    "    axes[0].annotate('Total: {:,d}'.format(np.sum(data)),(12,100),ha ='center',size=14)\n",
    "\n",
    "    plot_cluster(row_info_f,'cap_class', ax = axes[1])\n",
    "    plot_cluster(row_info_f,'style_class', ax = axes[2])\n",
    "    plot_cluster(row_info_f,'lipper_class', ax = axes[3])\n",
    "    plt.show()\n",
    "    style_map(row_info_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_grid(dictionary):\n",
    "    temp = pd.DataFrame([row for row in product(*dictionary.values())], \n",
    "                           columns=dictionary.keys())\n",
    "    return(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average_score(param_grid, relevant_params, measures):\n",
    "\n",
    "    param_grid = param_grid.fillna(value=0)\n",
    "    param_grid['param_id'] = (param_grid\n",
    "                                  .groupby(relevant_params)\n",
    "                                  .ngroup())\n",
    "    \n",
    "    # Fix for issue with same param_id for lipper rows\n",
    "    lipper_rows = param_grid.loc[param_grid['algo'] == 'lipper',:].copy()\n",
    "    lipper_rows['param_id'] = lipper_rows.groupby(['preprocessing']).ngroup()\n",
    "    lipper_rows['param_id'] = (lipper_rows['param_id'] + 1) * -1\n",
    "    param_grid.loc[param_grid['algo'] == 'lipper'] = lipper_rows\n",
    "\n",
    "    scores = param_grid[measures]\n",
    "    params_only = param_grid.drop(columns = measures)\n",
    "    \n",
    "    weights = (param_grid[['year','count']]\n",
    "                                  .drop_duplicates()\n",
    "                                  .assign(weight = lambda x: x['count'] / np.sum(x['count'])))\n",
    "    weights = weights[['weight']].values\n",
    "\n",
    "    scores = scores.groupby(params_only['param_id']).apply(lambda x: np.sum(x * weights))\n",
    "\n",
    "    params_only = (params_only\n",
    "                      .drop_duplicates(relevant_params)\n",
    "                      .drop(columns = ['year','count']))\n",
    "\n",
    "    result = params_only.merge(scores, how = 'left', on = 'param_id')\n",
    "    \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_algo(param_grid):\n",
    "    \n",
    "    # Setup\n",
    "    n_row = param_grid.shape[0]\n",
    "    cluster_list = []\n",
    "    result_grid = param_grid.copy()\n",
    "    \n",
    "    # Loop over all supplyed params\n",
    "    print('Start with params...')\n",
    "    for i, param in param_grid.iterrows():\n",
    "        row_info_f, returns_f, holdings_ft, begin_date, end_date = filter_data(param)\n",
    "        \n",
    "        algo = param['algo']\n",
    "        if(algo == 'spectral'): clustering = spectral_clustering(holdings_ft, param)\n",
    "        if(algo == 'kmeans'):   clustering = kmeans_clustering(holdings_ft, param)\n",
    "        if(algo == 'som'):      clustering = som_clustering(holdings_ft, param)\n",
    "\n",
    "        row_info_f = row_info_f.assign(cluster = clustering.labels_)\n",
    "        cluster_list.append(clustering.labels_)\n",
    "\n",
    "        db_score = davies_bouldin_score(holdings_ft.toarray(), row_info_f['cluster'])\n",
    "        s_score = silhouette_score(holdings_ft, row_info_f['cluster'])\n",
    "\n",
    "        result_grid.loc[i,'count'] = row_info_f.shape[0]\n",
    "        result_grid.loc[i,'score db'] = db_score\n",
    "        result_grid.loc[i,'score silhouette'] = s_score\n",
    "\n",
    "        # sim_results = simulation(row_info_f, returns_f, n_iterations = 500)\n",
    "        # result_grid.loc[i,'sim mret'] = sim_results[0]\n",
    "        # result_grid.loc[i,'sim std'] = sim_results[1]\n",
    "\n",
    "        returns_m, style_returns = calc_styleadj_returns(row_info_f, returns_f, style_cols='cluster')\n",
    "        result_grid.loc[i,'median tevola'] = error_vola_describe(returns_m)['50%'][0]\n",
    "        \n",
    "        progress = (i+1) / n_row * 100\n",
    "        print('Progress:                                          {:<5.2f}%'.format(progress))\n",
    "    \n",
    "\n",
    "    # Evaluate standart lipper classification\n",
    "    print('Evaluate Lipper clusters...')\n",
    "    years = param_grid['year'].unique()\n",
    "    preprocessing = param_grid['preprocessing'].unique()\n",
    "\n",
    "    lipper_grid = pd.DataFrame()\n",
    "    param_lipper = dict(\n",
    "                    year = years,\n",
    "                    preprocessing = preprocessing,\n",
    "                    verbose = [False]\n",
    "                    )\n",
    "    param_grid_lipper = expand_grid(param_lipper)\n",
    "        \n",
    "    for i, param_lipper in param_grid_lipper.iterrows():\n",
    "        row_info_f, returns_f, holdings_ft, begin_date, end_date = filter_data(param_lipper)\n",
    "        row_info_f = row_info_f.assign(cluster = row_info_f['lipper_class_num'])\n",
    "        cluster_list.append(row_info_f['lipper_class_num'])\n",
    "\n",
    "        db_score = davies_bouldin_score(holdings_ft.toarray(), row_info_f['cluster'])\n",
    "        s_score = silhouette_score(holdings_ft, row_info_f['cluster'])\n",
    "\n",
    "        lipper_grid.loc[i,'year'] = param_lipper['year']\n",
    "        lipper_grid.loc[i,'count'] = row_info_f.shape[0]\n",
    "        lipper_grid.loc[i,'score db'] = db_score\n",
    "        lipper_grid.loc[i,'score silhouette'] = s_score\n",
    "        lipper_grid.loc[i,'algo'] = 'lipper'\n",
    "        lipper_grid.loc[i,'preprocessing'] = param_lipper['preprocessing']\n",
    "\n",
    "        # sim_results = simulation(row_info_f, returns_f, n_iterations = 500)\n",
    "        # lipper_grid.loc[i,'sim mret'] = sim_results[0]\n",
    "        # lipper_grid.loc[i,'sim std'] = sim_results[1]\n",
    "        \n",
    "        returns_m, style_returns = calc_styleadj_returns(row_info_f, returns_f, style_cols='cluster')\n",
    "        lipper_grid.loc[i,'median tevola'] = error_vola_describe(returns_m)['50%'][0]\n",
    "        \n",
    "    # Concat and calc year weighted averages\n",
    "    param_grid_full = pd.concat([result_grid,lipper_grid], axis = 0, sort=False)\n",
    "    result = weighted_average_score(param_grid_full,\n",
    "                                    relevant_params = ['n_clusters','assign_labels',\n",
    "                                                       'affinity','gamma','n_init','algo','preprocessing'],\n",
    "                                    measures = ['score db', 'score silhouette', 'median tevola'])\n",
    "    result = result.drop(columns = ['verbose'])\n",
    "    result['years'] = '{} - {}'.format(np.min(years), np.max(years))\n",
    "    print('                                              ... Finished')\n",
    "\n",
    "    return(result, cluster_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018]\n",
    "param = dict(\n",
    "    year             = [2014],          # Integer or string 'full' for the whole sample\n",
    "    algo             = ['kmeans','spectral','som'],    # 'kmeans','spectral','som'\n",
    "    n_clusters       = [10], \n",
    "    preprocessing    = ['l1','l2'], \n",
    "    \n",
    "    # K-means and SOM\n",
    "    n_init           = [10],             # N init of kmeans\n",
    "    \n",
    "    # spectral specific\n",
    "    assign_labels    = ['kmeans'],      # 'kmeans' or 'discretize'\n",
    "    affinity         = ['rbf'],         # One of: rbf, nearest_neighbors\n",
    "    gamma            = [1],             # Sigma for rbf kernal\n",
    "\n",
    "    # som specific\n",
    "    training_epochs  = [1_000],\n",
    "    \n",
    "    verbose          = [False]\n",
    ")\n",
    "\n",
    "param_grid = expand_grid(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, cluster_list = full_algo(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_index = 3\n",
    "cluster = cluster_list[model_index]\n",
    "param = param_grid.iloc[model_index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_info_f, returns_f, holdings_ft, begin_date, end_date = filter_data(param)\n",
    "row_info_f = row_info_f.assign(cluster = cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_style(\"ticks\", {\"xtick.major.size\": 14, \"ytick.major.size\": 14})\n",
    "\n",
    "plot_cluster_wrapper(row_info_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize = (14,4))\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "sns.lineplot(data = result ,x = 'n_clusters', y='DB score', color='g', ax=ax1)\n",
    "sns.lineplot(data = result ,x = 'n_clusters', y='Silhouette score', ax=ax2)\n",
    "\n",
    "ax1.set_xlabel('n_cluster')\n",
    "ax1.set_ylabel('DB score', color='g')\n",
    "ax2.set_ylabel('Slihouette score', color='b')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = holdings_ft.shape[0]\n",
    "\n",
    "raw_data = holdings_ft[0:size]\n",
    "\n",
    "graph_data = kneighbors_graph(raw_data, \n",
    "                              n_neighbors = 10,\n",
    "                              mode = 'distance',\n",
    "                              p = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "cluster_encoded = le.fit_transform(row_info_f['cluster'])\n",
    "\n",
    "cluster = cluster_encoded\n",
    "cluster_text = row_info_f.loc[:size,'cluster']\n",
    "\n",
    "distance = graph_data.data\n",
    "vertex_n = graph_data.shape[0]\n",
    "\n",
    "x, y = graph_data.nonzero()\n",
    "\n",
    "g = Graph(directed=False)\n",
    "g.add_vertex(vertex_n)\n",
    "\n",
    "for s , t in zip(x,y):\n",
    "    g.add_edge(g.vertex(s), g.vertex(t))\n",
    "\n",
    "cluster_text.values\n",
    "\n",
    "v_cluster = g.new_vertex_property('int', vals = cluster)\n",
    "v_text = g.new_vertex_property(\"string\", vals = cluster_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_distance = g.new_edge_property('double', vals = distance)\n",
    "e_distance = prop_to_size(e_distance, mi=1, ma=10, log=True, power=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = sfdp_layout(g, eweight = e_distance, p = 1, C = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.iloc[model_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_draw(g, pos = pos,\n",
    "           output_size = (1000,1000), \n",
    "           #output = 'test.pdf',\n",
    "           vprops = {#'size' : 1,\n",
    "                     'color' : 'black',\n",
    "                     'fill_color' : v_cluster,\n",
    "                     'text' : v_text,\n",
    "                     'text_color' : 'white',\n",
    "                     'font_size' : 15,\n",
    "                     'font_weight' : cairo.FONT_WEIGHT_BOLD,\n",
    "                     'halo' : False,\n",
    "                     'halo_color' : v_cluster,\n",
    "                     'halo_size' : 1\n",
    "                    },\n",
    "           eprops = {'pen_width' : 0.2,\n",
    "                     'color' : 'grey'}\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Description\" data-toc-modified-id=\"Description-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Description</a></span></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setup</a></span></li><li><span><a href=\"#Other-functions\" data-toc-modified-id=\"Other-functions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Other functions</a></span></li><li><span><a href=\"#Clustering\" data-toc-modified-id=\"Clustering-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Clustering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Options\" data-toc-modified-id=\"Options-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Options</a></span></li><li><span><a href=\"#Spectral\" data-toc-modified-id=\"Spectral-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Spectral</a></span></li><li><span><a href=\"#K-means\" data-toc-modified-id=\"K-means-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>K-means</a></span></li></ul></li><li><span><a href=\"#Visualize\" data-toc-modified-id=\"Visualize-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Visualize</a></span></li><li><span><a href=\"#Chart-the-returns-of-the-formed-clustes\" data-toc-modified-id=\"Chart-the-returns-of-the-formed-clustes-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Chart the returns of the formed clustes</a></span></li><li><span><a href=\"#Analysing-clusters\" data-toc-modified-id=\"Analysing-clusters-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Analysing clusters</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "MT",
   "language": "python",
   "name": "mt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "notify_time": "5",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
