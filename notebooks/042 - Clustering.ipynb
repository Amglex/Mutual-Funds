{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Description\" data-toc-modified-id=\"Description-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Description</a></span></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setup</a></span></li><li><span><a href=\"#Other-functions\" data-toc-modified-id=\"Other-functions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Other functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Clustering\" data-toc-modified-id=\"Clustering-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Clustering</a></span></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Evaluation</a></span></li><li><span><a href=\"#Plotting\" data-toc-modified-id=\"Plotting-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Plotting</a></span></li><li><span><a href=\"#Grid-Wrapper\" data-toc-modified-id=\"Grid-Wrapper-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Grid Wrapper</a></span></li></ul></li><li><span><a href=\"#Clustering\" data-toc-modified-id=\"Clustering-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Clustering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Options\" data-toc-modified-id=\"Options-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Options</a></span></li></ul></li><li><span><a href=\"#Selected-Model\" data-toc-modified-id=\"Selected-Model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Selected Model</a></span></li><li><span><a href=\"#Visualize\" data-toc-modified-id=\"Visualize-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Visualize</a></span><ul class=\"toc-item\"><li><span><a href=\"#Graph\" data-toc-modified-id=\"Graph-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Graph</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cluster funds using Spektral Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import feather\n",
    "from itertools import product\n",
    "from multiprocessing import Pool\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import sparse\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "from minisom import MiniSom\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from sklearn.preprocessing import normalize, LabelEncoder, minmax_scale\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score, davies_bouldin_score\n",
    "from sklearn.neighbors import kneighbors_graph, KNeighborsClassifier\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "from graph_tool.all import *\n",
    "import cairo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/processed/full.pickle'\n",
    "pickle_off = open(path,\"rb\")\n",
    "dict_all_years = pickle.load(pickle_off)\n",
    "dict_year = dict_all_years[2018]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_clustering(holdings_ft, param):\n",
    "    verbose = param['verbose']\n",
    "\n",
    "    if(verbose): print('Start clustering...')\n",
    "    clustering = SpectralClustering(n_clusters = param['n_clusters'],\n",
    "                                    assign_labels = param['assign_labels'], # kmeans or discretize\n",
    "                                    eigen_solver = 'arpack',\n",
    "                                    affinity = param['affinity'],\n",
    "                                    gamma = param['gamma'],\n",
    "                                    n_init = param['n_init'],\n",
    "                                    n_jobs = -1,\n",
    "                                    random_state = 0).fit(holdings_ft)\n",
    "    if(verbose): print('Clustering finished')\n",
    "    \n",
    "    return(clustering.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_clustering(holdings_ft, param):\n",
    "    verbose = param['verbose']\n",
    "\n",
    "    if(verbose): print('Start clustering...')\n",
    "    clustering = KMeans(n_clusters = param['n_clusters'],\n",
    "                        verbose = verbose,\n",
    "                        n_init = param['n_init'], # Number of runs\n",
    "                        n_jobs= -1,\n",
    "                        random_state = 1\n",
    "                       ).fit(holdings_ft)\n",
    "    \n",
    "    if(verbose): print('Clustering finished')\n",
    "    \n",
    "    return(clustering.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def som_clustering(holdings_ft, param):\n",
    "    verbose = param['verbose']\n",
    "    if(verbose): print('Start clustering...')\n",
    "    \n",
    "    ### Initialization and training ###\n",
    "    # Configure SOM\n",
    "    som = MiniSom(x = 25,\n",
    "                  y = 25,\n",
    "                  input_len = holdings_ft.shape[1],\n",
    "                  sigma = 1.0,\n",
    "                  learning_rate = 0.5)\n",
    "\n",
    "    # Initialize\n",
    "    data = holdings_ft.toarray()\n",
    "    som.random_weights_init(data)\n",
    "\n",
    "    # Train\n",
    "    som.train_random(data, param['training_epochs'], verbose = verbose) # training with 100 iterations\n",
    "    som_quantized = som.quantization(data)\n",
    "\n",
    "    clustering = KMeans(n_clusters = param['n_clusters'],\n",
    "                        verbose = verbose,\n",
    "                        n_init = param['n_init'], # Number of runs\n",
    "                        n_jobs= -1,\n",
    "                        random_state = 1\n",
    "                       ).fit(som_quantized)\n",
    "\n",
    "    if(verbose): print('Clustering finished')    \n",
    "    return(clustering.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_styleadj_returns(row_info_f, returns_f, style_cols):\n",
    "\n",
    "    row_info_m = row_info_f.copy()\n",
    "    returns_m = returns_f.copy()\n",
    "    \n",
    "    returns_m = returns_m.sort_values(['crsp_fundno','report_dt'])\n",
    "\n",
    "    # merge predicted styles onto returns\n",
    "    returns_m = returns_m.merge(row_info_m[['crsp_fundno', 'report_dt', style_cols]],\n",
    "                            how='left',\n",
    "                            on=['crsp_fundno', 'report_dt'])\n",
    "\n",
    "    # Forward fill all styles and drop nas\n",
    "    returns_m = (returns_m.apply(lambda x: x.fillna(method = 'ffill'))\n",
    "    )\n",
    "\n",
    "    # Calc mean return per style\n",
    "    style_returns = (returns_m\n",
    "                        .groupby([style_cols,'report_dt'])\n",
    "                        .mean()\n",
    "                        .reset_index()\n",
    "                        .drop(columns='crsp_fundno')\n",
    "    )\n",
    "\n",
    "    # Merge style returns onto fund returns and calc tracking error\n",
    "    returns_m = (returns_m\n",
    "                        .rename(columns = {'mret' : 'fund_ret'}) \n",
    "                        .merge(style_returns,\n",
    "                                    how = 'left',\n",
    "                                    on = [style_cols,'report_dt'])\n",
    "                        .assign(error = lambda df: df['fund_ret'] - df['mret'])\n",
    "                        .rename(columns = {'mret' : 'style_ret'}) \n",
    "    )\n",
    "\n",
    "    returns_m = returns_m[['crsp_fundno', 'report_dt', style_cols,\n",
    "                       'fund_ret', 'style_ret', 'error']]\n",
    "\n",
    "    return(returns_m, style_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_vola_deciles(returns_m): \n",
    "    error_vol = (returns_m\n",
    "                 .groupby(['crsp_fundno'])['error']\n",
    "                 .std()\n",
    "                 .reset_index())\n",
    "    error_vol['error'] = error_vol['error'] * 100\n",
    "    \n",
    "    error_vol = (error_vol[['error']]\n",
    "                .apply(lambda x : x.quantile(np.round(np.arange(0.1,1,0.1),2))))\n",
    "    return(error_vol)\n",
    "\n",
    "def error_vola_describe(returns_m): \n",
    "    error_vol = (returns_m\n",
    "                 .groupby(['crsp_fundno'])['error']\n",
    "                 .std()\n",
    "                 .reset_index())\n",
    "    error_vol['error'] = error_vol['error'] * 100\n",
    "    return(error_vol[['error']].describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation(row_info_f, returns_f, n_iterations):\n",
    "    np.random.seed()\n",
    "\n",
    "    #n_iterations = 500\n",
    "    \n",
    "    # First choose n samples of funds with one fund per cluster\n",
    "    funds_list = []\n",
    "    cluster = np.array(row_info_f[['crsp_fundno','cluster']])\n",
    "    arr = np.arange(row_info_f.shape[0])\n",
    "\n",
    "    for i in np.arange(n_iterations):\n",
    "        np.random.shuffle(arr)\n",
    "        cluster = cluster[arr]\n",
    "        index = np.unique(cluster[:,1], return_index = True, return_inverse = False)[1]\n",
    "        funds = cluster[index,0]\n",
    "        funds_list.append(funds)\n",
    "\n",
    "\n",
    "    mean_return = []\n",
    "    mean_std = []\n",
    "    returns_fundnos = returns_f['crsp_fundno'].values\n",
    "\n",
    "    for funds in funds_list:\n",
    "        # Take returns for sample and calc equally weighted average return\n",
    "        returns_index = np.isin(returns_fundnos,funds)\n",
    "        returns_s = returns_f[returns_index]\n",
    "        returns_s = returns_s.groupby('report_dt')['mret'].mean()\n",
    "\n",
    "        # Calc mean and std\n",
    "        mean_return.append(returns_s.std())\n",
    "        mean_std.append(returns_s.mean())\n",
    "\n",
    "\n",
    "    mean_return = pd.DataFrame(mean_return).mean()\n",
    "    mean_std = pd.DataFrame(mean_std).mean()\n",
    "    \n",
    "    return([mean_return[0], mean_std[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation_wrapper(row_info_f, returns_f, n_iterations):\n",
    "    result_list = []\n",
    "    pool = Pool()\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        pool.apply_async(simulation, callback = result_list.append)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    result = result_list\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_map(row_info_f):\n",
    "    cap = [0,1,2,3]\n",
    "    style = [0,1,2]\n",
    "\n",
    "    counts = row_info_f['cluster'].value_counts().sort_index()\n",
    "    size = minmax_scale(counts,feature_range=(1,2)) * 20\n",
    "\n",
    "    data = round(\n",
    "            pd.crosstab(\n",
    "                row_info_f['cap_class'],row_info_f['cluster'], \n",
    "                margins = True, normalize = 'columns') * 100, 2).T\n",
    "\n",
    "    x = data.apply(lambda x: np.sum(x * cap) / 100, axis = 1)\n",
    "\n",
    "    data = round(\n",
    "            pd.crosstab(\n",
    "                row_info_f['style_class'],row_info_f['cluster'], \n",
    "                margins = True, normalize = 'columns') * 100, 2).T\n",
    "    y = data.apply(lambda x: np.sum(x * style) / 100, axis = 1)\n",
    "\n",
    "    label = x.index[:-1]\n",
    "\n",
    "    fig = plt.figure(figsize=(15,4))\n",
    "    ax_s = fig.add_subplot(111)\n",
    "\n",
    "    #ax_s.grid(True)\n",
    "\n",
    "    plt.xlabel('Market cap dimension')\n",
    "    plt.xticks([0,1,2,3], ['SC','MC','ML','LC'])\n",
    "\n",
    "    plt.ylabel('Style dimension')\n",
    "    plt.yticks([0,1,2], ['V','C','G'])\n",
    "\n",
    "    for i, txt in enumerate(label):\n",
    "        ax_s.annotate(txt, (x[i], y[i]),\n",
    "                     xytext = (0, 0),              # Horizontally shift label by `space`\n",
    "                     textcoords = 'offset points', # Interpret `xytext` as offset in points\n",
    "                     va='center',                  # Vertically center label\n",
    "                     ha='center',\n",
    "                     color = 'black',\n",
    "                     size = size[i])  \n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def plot_cluster(row_info_f, style, ax):\n",
    "    data = round(\n",
    "        pd.crosstab(\n",
    "            row_info_f[style],row_info_f['cluster'], \n",
    "            margins = True, normalize = 'columns') * 100, 2).T\n",
    "\n",
    "    data.plot(kind='bar', \n",
    "                 stacked=True, ax = ax)\n",
    "\n",
    "    ax.legend().remove()\n",
    "    label_list = data.columns.values.astype(str).repeat(data.shape[0])\n",
    "    rects = ax.patches\n",
    "\n",
    "    # For each bar: Place a label\n",
    "    for i, rect in enumerate(rects):\n",
    "        if rect.get_height() > 10:\n",
    "            # Get X and Y placement of label from rect.\n",
    "            x_value = rect.get_x() + rect.get_width() / 2\n",
    "            y_value = rect.get_y() + rect.get_height() / 2\n",
    "\n",
    "            # Use X value as label and format number with one decimal place\n",
    "            label = \"{}\".format(label_list[i])\n",
    "\n",
    "            # Create annotation\n",
    "            ax.annotate(\n",
    "                label,                        # Use `label` as label\n",
    "                (x_value, y_value),           # Place label at end of the bar\n",
    "                xytext = (0, 0),              # Horizontally shift label by `space`\n",
    "                textcoords = 'offset points', # Interpret `xytext` as offset in points\n",
    "                va='center',                  # Vertically center label\n",
    "                ha='center',\n",
    "                color = 'white',\n",
    "                size = 12)                  # Horizontally align label \n",
    "    return(ax)\n",
    "    \n",
    "def plot_cluster_wrapper(row_info_f):\n",
    "    \n",
    "    f, axes = plt.subplots(nrows = 4, ncols=1, sharex=True, \n",
    "                           figsize = (15,6), gridspec_kw={'height_ratios':[1,2,2,2]})\n",
    "    \n",
    "    data = row_info_f['cluster'].value_counts().sort_index().append(to_append = pd.Series([0]))\n",
    "    data.plot(kind='bar', ax = axes[0])\n",
    "    axes[0].annotate('Total: {:,d}'.format(np.sum(data)),(12,100),ha ='center',size=14)\n",
    "\n",
    "    plot_cluster(row_info_f,'cap_class', ax = axes[1])\n",
    "    plot_cluster(row_info_f,'style_class', ax = axes[2])\n",
    "    plot_cluster(row_info_f,'lipper_class', ax = axes[3])\n",
    "    plt.show()\n",
    "    style_map(row_info_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_grid(dictionary):\n",
    "    temp = pd.DataFrame([row for row in product(*dictionary.values())], \n",
    "                           columns=dictionary.keys())\n",
    "    return(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average_score(param_grid, relevant_params, measures):\n",
    "\n",
    "    param_grid = param_grid.fillna(value=0)\n",
    "    param_grid['param_id'] = (param_grid\n",
    "                                  .groupby(relevant_params)\n",
    "                                  .ngroup())\n",
    "    \n",
    "    # Fix for issue with same param_id for lipper rows\n",
    "    lipper_rows = param_grid.loc[param_grid['algo'] == 'lipper',:].copy()\n",
    "    lipper_rows['param_id'] = lipper_rows.groupby(['preprocessing']).ngroup()\n",
    "    lipper_rows['param_id'] = (lipper_rows['param_id'] + 1) * -1\n",
    "    param_grid.loc[param_grid['algo'] == 'lipper'] = lipper_rows\n",
    "\n",
    "    scores = param_grid[measures]\n",
    "    params_only = param_grid.drop(columns = measures)\n",
    "    \n",
    "    weights = (param_grid[['year','count']]\n",
    "                                  .drop_duplicates()\n",
    "                                  .assign(weight = lambda x: x['count'] / np.sum(x['count'])))\n",
    "    weights = weights[['weight']].values\n",
    "\n",
    "    scores = scores.groupby(params_only['param_id']).apply(lambda x: np.sum(x * weights))\n",
    "\n",
    "    params_only = (params_only\n",
    "                      .drop_duplicates(relevant_params)\n",
    "                      .drop(columns = ['year','count']))\n",
    "\n",
    "    result = params_only.merge(scores, how = 'left', on = 'param_id')\n",
    "    \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_algo(param_grid):\n",
    "    \n",
    "    # Setup\n",
    "    n_row = param_grid.shape[0]\n",
    "    cluster_list = []\n",
    "    result_grid = param_grid.copy()\n",
    "    \n",
    "    # Loop over all supplyed params\n",
    "    print('Start with params...')\n",
    "    for i, param in param_grid.iterrows():\n",
    "        \n",
    "        row_info_f = dict_all_years[param['year']]['row_info_f']\n",
    "        returns_f = dict_all_years[param['year']]['returns_f']\n",
    "        holdings_ft = dict_all_years[param['year']]['holdings_ft']\n",
    "        begin_date = dict_all_years[param['year']]['begin_date']\n",
    "        end_date = dict_all_years[param['year']]['end_date']\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "        row_info_f['lipper_class_num'] = le.fit_transform(row_info_f['lipper_class'])\n",
    "        \n",
    "        algo = param['algo']\n",
    "        \n",
    "        if(algo == 'spectral'): cluster = spectral_clustering(holdings_ft, param)\n",
    "        if(algo == 'kmeans'):   cluster = kmeans_clustering(holdings_ft, param)\n",
    "        if(algo == 'som'):      cluster = som_clustering(holdings_ft, param)\n",
    "        \n",
    "        row_info_f = row_info_f.assign(cluster = cluster)\n",
    "        cluster_list.append(row_info_f)\n",
    "\n",
    "        db_score = davies_bouldin_score(holdings_ft.toarray(), row_info_f['cluster'])\n",
    "        # s_score = silhouette_score(holdings_ft, row_info_f['cluster'])\n",
    "\n",
    "        result_grid.loc[i,'count'] = row_info_f.shape[0]\n",
    "        result_grid.loc[i,'score db'] = db_score\n",
    "        # result_grid.loc[i,'score silhouette'] = s_score\n",
    "\n",
    "        # sim_results = simulation(row_info_f, returns_f, n_iterations = 500)\n",
    "        # result_grid.loc[i,'sim mret'] = sim_results[0]\n",
    "        # result_grid.loc[i,'sim std'] = sim_results[1]\n",
    "\n",
    "        # returns_m, style_returns = calc_styleadj_returns(row_info_f, returns_f, style_cols='cluster')\n",
    "        # result_grid.loc[i,'median tevola'] = error_vola_describe(returns_m)['50%'][0]\n",
    "        \n",
    "        progress = (i+1) / n_row * 100\n",
    "        print('Progress:                                          {:<5.2f}%'.format(progress))\n",
    "    \n",
    "\n",
    "    # Evaluate standart lipper classification\n",
    "    print('Evaluate Lipper clusters...')\n",
    "    years = param_grid['year'].unique()\n",
    "    preprocessing = param_grid['preprocessing'].unique()\n",
    "\n",
    "    lipper_grid = pd.DataFrame()\n",
    "    param_lipper = dict(\n",
    "                    year = years,\n",
    "                    preprocessing = preprocessing,\n",
    "                    verbose = [False]\n",
    "                    )\n",
    "    param_grid_lipper = expand_grid(param_lipper)\n",
    "        \n",
    "    for i, param_lipper in param_grid_lipper.iterrows():\n",
    "\n",
    "        row_info_f = dict_all_years[param_lipper['year']]['row_info_f']\n",
    "        returns_f = dict_all_years[param_lipper['year']]['returns_f']\n",
    "        holdings_ft = dict_all_years[param_lipper['year']]['holdings_ft']\n",
    "        begin_date = dict_all_years[param_lipper['year']]['begin_date']\n",
    "        end_date = dict_all_years[param_lipper['year']]['end_date']\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "        row_info_f['lipper_class_num'] = le.fit_transform(row_info_f['lipper_class'])\n",
    "        \n",
    "        row_info_f = row_info_f.assign(cluster = row_info_f['lipper_class_num'])\n",
    "        #cluster_list.append(row_info_f['lipper_class_num'])\n",
    "\n",
    "        db_score = davies_bouldin_score(holdings_ft.toarray(), row_info_f['cluster'])\n",
    "        # s_score = silhouette_score(holdings_ft, row_info_f['cluster'])\n",
    "\n",
    "        lipper_grid.loc[i,'year'] = param_lipper['year']\n",
    "        lipper_grid.loc[i,'count'] = row_info_f.shape[0]\n",
    "        lipper_grid.loc[i,'score db'] = db_score\n",
    "        # lipper_grid.loc[i,'score silhouette'] = s_score\n",
    "        lipper_grid.loc[i,'algo'] = 'lipper'\n",
    "        lipper_grid.loc[i,'preprocessing'] = param_lipper['preprocessing']\n",
    "\n",
    "        # sim_results = simulation(row_info_f, returns_f, n_iterations = 500)\n",
    "        # lipper_grid.loc[i,'sim mret'] = sim_results[0]\n",
    "        # lipper_grid.loc[i,'sim std'] = sim_results[1]\n",
    "        \n",
    "        # returns_m, style_returns = calc_styleadj_returns(row_info_f, returns_f, style_cols='cluster')\n",
    "        # lipper_grid.loc[i,'median tevola'] = error_vola_describe(returns_m)['50%'][0]\n",
    "        \n",
    "    # Concat and calc year weighted averages\n",
    "    param_grid_full = pd.concat([result_grid,lipper_grid], axis = 0, sort=False)\n",
    "    result = weighted_average_score(param_grid_full,\n",
    "                                    relevant_params = ['n_clusters','assign_labels',\n",
    "                                                       'affinity','gamma','n_init','algo','preprocessing',\n",
    "                                                        'n_neighbors', 'distance_param', 'weights'],\n",
    "                                    #measures = ['score db', 'score silhouette', 'median tevola'])\n",
    "                                    measures = ['score db'])\n",
    "    result = result.drop(columns = ['verbose'])\n",
    "    result['years'] = '{} - {}'.format(np.min(years), np.max(years))\n",
    "    print('                                              ... Finished')\n",
    "\n",
    "    return(result, cluster_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018]\n",
    "param = dict(\n",
    "    year             = [2018],          # Integer or string 'full' for the whole sample\n",
    "    algo             = ['kmeans'],    # 'kmeans','spectral','som'\n",
    "    n_clusters       = [2,4,6,8,10,12,14,16,18,20], \n",
    "    preprocessing    = ['l2'], \n",
    "    \n",
    "    # K-means and SOM\n",
    "    n_init           = [100],             # N init of kmeans\n",
    "    \n",
    "    # spectral specific\n",
    "    assign_labels    = ['kmeans'],      # 'kmeans' or 'discretize'\n",
    "    affinity         = ['rbf'],         # One of: rbf, nearest_neighbors\n",
    "    gamma            = [1],             # Sigma for rbf kernal\n",
    "\n",
    "    # som specific\n",
    "    training_epochs  = [5_000],\n",
    "    \n",
    "    # Classification\n",
    "    n_neighbors     = [10],\n",
    "    distance_param  = [2],                   # 1: manhattan distance, 2: euclidian distance\n",
    "    weights         = ['distance'],          # One of 1: (distance) or 2: (uniform)\n",
    "    \n",
    "    verbose          = [True]\n",
    ")\n",
    "\n",
    "param_grid = expand_grid(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, cluster_list = full_algo(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_temp_a = result.drop(columns = ['assign_labels','affinity', 'gamma', 'training_epochs']).sort_values(['n_clusters']).iloc[:,[1,8]]\n",
    "result_temp_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_temp_a = result.drop(columns = ['assign_labels','affinity', 'gamma', 'training_epochs']).sort_values(['n_clusters']).iloc[:,[1,8]]\n",
    "result_temp_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.drop(columns = ['assign_labels','affinity', 'gamma', 'training_epochs']).sort_values(['score db'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(result_temp_a.T,2).to_latex(index = False,\n",
    "                    index_names = False,\n",
    "                    bold_rows = False)\n",
    "     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.loc[result.algo == 'lipper', 'score db'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table = result.loc[:,['n_clusters','score db']]\n",
    "result_table.columns = ['n clusters', 'k-means']\n",
    "result_table = result_table.iloc[:-1,:]\n",
    "result_table.loc[:,'Lipper class'] = result.loc[result.algo == 'lipper', 'score db'].values[0]\n",
    "result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize = (14,4))\n",
    "\n",
    "sns.lineplot(data = result_table ,x = 'n clusters', y='k-means', label='k-means model', ax=ax1)\n",
    "sns.lineplot(data = result_table ,x = 'n clusters', y='Lipper class',  label='Lipper class', ax=ax1)\n",
    "\n",
    "ax1.set_xlabel('k clusters')\n",
    "ax1.set_ylabel('DB score')\n",
    "\n",
    "plt.ylim(2, 8)\n",
    "plt.xlim(2,20)\n",
    "\n",
    "ax1.legend(loc=\"lower right\", frameon=False)\n",
    "\n",
    "plt.savefig('../reports/figures/graph/kmeans_cluster.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selected Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018]\n",
    "param = dict(\n",
    "    year             = [2018],          # Integer or string 'full' for the whole sample\n",
    "    algo             = ['som'],    # 'kmeans','spectral','som'\n",
    "    n_clusters       = [6], \n",
    "    preprocessing    = ['l2'], \n",
    "    \n",
    "    # K-means and SOM\n",
    "    n_init           = [250],             # N init of kmeans\n",
    "    \n",
    "    # spectral specific\n",
    "    assign_labels    = ['kmeans'],      # 'kmeans' or 'discretize'\n",
    "    affinity         = ['rbf'],         # One of: rbf, nearest_neighbors\n",
    "    gamma            = [1],             # Sigma for rbf kernal\n",
    "\n",
    "    # som specific\n",
    "    training_epochs  = [10_000],\n",
    "    \n",
    "    # Classification\n",
    "    n_neighbors     = [10],\n",
    "    distance_param  = [2],                   # 1: manhattan distance, 2: euclidian distance\n",
    "    weights         = ['distance'],          # One of 1: (distance) or 2: (uniform)\n",
    "    \n",
    "    verbose          = [True]\n",
    ")\n",
    "\n",
    "param_grid = expand_grid(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_specific, cluster_list_specific = full_algo(param_grid)\n",
    "\n",
    "style_df = pd.concat(cluster_list_specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.drop(columns = ['assign_labels','affinity', 'gamma', 'training_epochs']).sort_values(['score db'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition = pd.crosstab(style_df['style_class'], \n",
    "                    style_df['cluster'],\n",
    "                    normalize='index') * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(transition,2).to_latex(index = True,\n",
    "                    index_names = False,\n",
    "                    bold_rows = False)\n",
    "     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_index = 0\n",
    "cluster = cluster_list[model_index]\n",
    "param = param_grid.iloc[model_index,:]\n",
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_info_f = dict_all_years[param['year']]['row_info_f']\n",
    "returns_f = dict_all_years[param['year']]['returns_f']\n",
    "holdings_ft = dict_all_years[param['year']]['holdings_ft']\n",
    "begin_date = dict_all_years[param['year']]['begin_date']\n",
    "end_date = dict_all_years[param['year']]['end_date']\n",
    "        \n",
    "row_info_f = row_info_f.assign(cluster = cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "row_info_f['cluster'] = le.fit_transform(row_info_f['cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_style(\"ticks\", {\"xtick.major.size\": 14, \"ytick.major.size\": 14})\n",
    "\n",
    "plot_cluster_wrapper(row_info_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize = (14,4))\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "sns.lineplot(data = result ,x = 'n_clusters', y='DB score', color='g', ax=ax1)\n",
    "sns.lineplot(data = result ,x = 'n_clusters', y='Silhouette score', ax=ax2)\n",
    "\n",
    "ax1.set_xlabel('n_cluster')\n",
    "ax1.set_ylabel('DB score', color='g')\n",
    "ax2.set_ylabel('Slihouette score', color='b')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = holdings_ft.shape[0]\n",
    "\n",
    "raw_data = holdings_ft[0:size]\n",
    "\n",
    "graph_data = kneighbors_graph(raw_data, \n",
    "                              n_neighbors = 30,\n",
    "                              mode = 'distance',\n",
    "                              p = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "cluster_encoded = le.fit_transform(row_info_f['cluster'])\n",
    "\n",
    "cluster = cluster_encoded\n",
    "cluster_text = row_info_f.loc[:size,'cluster']\n",
    "\n",
    "distance = graph_data.data\n",
    "vertex_n = graph_data.shape[0]\n",
    "\n",
    "x, y = graph_data.nonzero()\n",
    "\n",
    "g = Graph(directed=False)\n",
    "g.add_vertex(vertex_n)\n",
    "\n",
    "for s , t in zip(x,y):\n",
    "    g.add_edge(g.vertex(s), g.vertex(t))\n",
    "\n",
    "cluster_text.values\n",
    "\n",
    "v_cluster = g.new_vertex_property('int', vals = cluster)\n",
    "v_text = g.new_vertex_property(\"string\", vals = cluster_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_distance = g.new_edge_property('double', vals = distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = sfdp_layout(g, eweight = e_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.iloc[model_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_draw(g, \n",
    "           output_size = (1000,1000), \n",
    "           #output = 'test.pdf',\n",
    "           vprops = {'size' : 15,\n",
    "                     'color' : 'black',\n",
    "                     'fill_color' : v_cluster,\n",
    "                     'text' : v_text,\n",
    "                     'text_color' : 'white',\n",
    "#                     'font_size' : 5,\n",
    "                     'font_weight' : cairo.FONT_WEIGHT_BOLD,\n",
    "                     'halo' : False\n",
    "                    },\n",
    "           eprops = {'pen_width' : 0.2,\n",
    "                     'color' : 'grey'}\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Description\" data-toc-modified-id=\"Description-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Description</a></span></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setup</a></span></li><li><span><a href=\"#Other-functions\" data-toc-modified-id=\"Other-functions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Other functions</a></span></li><li><span><a href=\"#Clustering\" data-toc-modified-id=\"Clustering-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Clustering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Options\" data-toc-modified-id=\"Options-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Options</a></span></li><li><span><a href=\"#Spectral\" data-toc-modified-id=\"Spectral-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Spectral</a></span></li><li><span><a href=\"#K-means\" data-toc-modified-id=\"K-means-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>K-means</a></span></li></ul></li><li><span><a href=\"#Visualize\" data-toc-modified-id=\"Visualize-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Visualize</a></span></li><li><span><a href=\"#Chart-the-returns-of-the-formed-clustes\" data-toc-modified-id=\"Chart-the-returns-of-the-formed-clustes-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Chart the returns of the formed clustes</a></span></li><li><span><a href=\"#Analysing-clusters\" data-toc-modified-id=\"Analysing-clusters-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Analysing clusters</a></span></li></ul></div>"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "MT",
   "language": "python",
   "name": "mt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "notify_time": "5",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
