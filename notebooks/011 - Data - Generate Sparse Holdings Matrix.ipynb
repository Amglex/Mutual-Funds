{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Large Holdings File\n",
    "\n",
    "#### Converting the raw 50+ GB sas file with the holdings complezte data into a sparse python matrix which can be loaded into memory and more important which can be handled more efficiently by different alogorithms. \n",
    "#### The logic behind this process is as follows:\n",
    "\n",
    "Loading data and transforming it into csv file to work with\n",
    "\n",
    "1. 50+ GB holdings.sas7bdat file containing all the holdings data downloaded directly from wrds using ftp client\n",
    "2. Converted into csv using sas7bdat_to_csv utility (Link)\n",
    "\n",
    "Two step process to transform file into sparse matrix\n",
    "Challenge is to convert from row describing one holding to rows describing the holdings of one fund at one point in time. Aslo it is crucial to keep track of which row of the sparse matrix is which fund at wjich date and which colums are which securities.\n",
    "\n",
    "3. Open file in python \n",
    "4. Parse through file to make two lists. One with all fund/date combinations (using the comination as an ID) and one with all securities.\n",
    "5. Generate sparse matrix with the dimensions \"number of fund/date combinations\" x \"numer of securities\"\n",
    "6. Parse through large csv file again and fill the percentage_tna (percentage of the fund held in that particular security) number into the right spot of the sparse matrix as determined by two maps based on all fund/date combinations and securities\n",
    "7. Save final sparse matrix and tables containing information about which row is which fund/date and which column is which security."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "Parsing through csv file could be significantly sped up using something like: https://stackoverflow.com/questions/17444679/reading-a-huge-csv-file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# add the 'src' directory as one where we can import modules\n",
    "src_dir = os.path.join(os.getcwd(), os.pardir, 'src')\n",
    "sys.path.append(src_dir)\n",
    "from data.basic_functions import * \n",
    "\n",
    "import csv\n",
    "import collections\n",
    "import feather\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For multiprocessing\n",
    "import multiprocessing\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find all unqiue Stocks and Portfolio/dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_port_id(a,b):\n",
    "    \"\"\"\n",
    "    Generate a unique ID from the portno and the date of a fund/date combination\n",
    "    \n",
    "    Input:\n",
    "    - a: port_no\n",
    "    - b: date\n",
    "    \n",
    "    Output:\n",
    "    - port_ID\n",
    "    \"\"\"\n",
    "    return int(100_000 * a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO\n",
    "Run again without break!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_reader(reader_object):\n",
    "    \"\"\"\n",
    "    Loops over rows of holdings csv\n",
    "    Needed to generate sparse matrix\n",
    "    \n",
    "    Input: \n",
    "    - a reader object linking to the holdings csv\n",
    "    \n",
    "    Output: \n",
    "    - port_ID: Collection of all unique fund/date combinations\n",
    "    - stock_ID: Collection of all unique stocks\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    \n",
    "    next(reader)\n",
    "    stocks = collections.Counter()\n",
    "    port_ID = collections.Counter()\n",
    "    \n",
    "    for row in reader_object:\n",
    "        port_ID_str = make_port_id(int(float(row[0])),int(float(row[1])))\n",
    "        port_ID[port_ID_str] += 1\n",
    "        stocks[int(float(row[7]))] += 1\n",
    "        count += 1\n",
    " #       if count == 10_000_000:\n",
    " #           break\n",
    "\n",
    "    \n",
    "    return(port_ID,stocks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/raw/out.csv'\n",
    "\n",
    "input_file = open(path)\n",
    "reader = csv.reader(input_file, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 47s, sys: 10.8 s, total: 12min 58s\n",
      "Wall time: 13min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "port_ID, stocks = extra_reader(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184578842"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(port_ID.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chunksize = 10 ** 8\n",
    "for chunk in pd.read_csv(path, chunksize=chunksize):\n",
    "    process(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create stock and port_no map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_unique_dict(counter):\n",
    "    \"\"\"\n",
    "    Used to make a dictionary linking each fund/date combination \n",
    "    and each stock to a row/col in the sparse matrix\n",
    "    \n",
    "    Input:\n",
    "    - collections.Counter() object\n",
    "    \n",
    "    Output:\n",
    "    - dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    unique_keys = list(counter.keys())\n",
    "    unique_keys_numbers = list(np.arange(len(unique_keys)))\n",
    "    counter_map = dict(zip(unique_keys, unique_keys_numbers))\n",
    "    \n",
    "    return(counter_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_map = make_unique_dict(stocks)\n",
    "port_no_map = make_unique_dict(port_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "738860"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(port_no_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows in file:  184,578,842\n",
      "Numer of unique stocks:        2,382,968\n",
      "Numer of unique portfolios:    738,860\n"
     ]
    }
   ],
   "source": [
    "total_number_rows = sum(list(stocks.values()))\n",
    "print('Total number of rows in file:  {:,}'.format(total_number_rows))\n",
    "print('Numer of unique stocks:        {:,}'.format(len(stocks.keys())))\n",
    "print('Numer of unique portfolios:    {:,}'.format(len(port_no_map.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse through file and create data for sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sparse_data(reader):\n",
    "    \"\"\"\n",
    "    Loop over holdings csv file to collect the data for the sparse matrix\n",
    "    \n",
    "    Input:\n",
    "    - reader: CSV holdings file\n",
    "    \n",
    "    Output:\n",
    "    - sparse_row, sparse_col, sparse_data: three np arrays for the construction of the sparse matrix\n",
    "    \"\"\"\n",
    "    next(reader)\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    sparse_row = np.zeros(total_number_rows)\n",
    "    sparse_col = np.zeros(total_number_rows)\n",
    "    sparse_data = np.zeros(total_number_rows)\n",
    "    \n",
    "    for row in reader:\n",
    "        # Row\n",
    "        port_ID = make_port_id(int(float(row[0])),int(float(row[1])))\n",
    "        sparse_row[counter] = port_no_map[port_ID]\n",
    "\n",
    "        # Col\n",
    "        stock_num = int(float(row[7]))\n",
    "        sparse_col[counter] = stock_map[stock_num]\n",
    "\n",
    "        # Data\n",
    "        try:\n",
    "            sparse_data[counter] = float(row[4])\n",
    "        except: \n",
    "            sparse_data[counter] = 0\n",
    "            \n",
    "        counter += 1\n",
    "    \n",
    "    sparse_row = sparse_row.astype(int)\n",
    "    sparse_col = sparse_col.astype(int)\n",
    "    \n",
    "    return(sparse_row, sparse_col, sparse_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/raw/out.csv'\n",
    "input_file = open(path)\n",
    "reader = csv.reader(input_file, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 21s, sys: 14.7 s, total: 13min 36s\n",
      "Wall time: 13min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sparse_row, sparse_col, sparse_data = gen_sparse_data(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up sparse matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important to use masks on sparse matrix and row/colums info tables to keep everything correctly aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'row': sparse_row, 'col' : sparse_col, 'data' :sparse_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of rawdata used to generate sparse matrix: 184,578,842, 3\n"
     ]
    }
   ],
   "source": [
    "print('Shape of rawdata used to generate sparse matrix: {:,}, {:,}'.format(data.shape[0], data.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fist mask: drop duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO \n",
    "\n",
    "check how many and why they exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1_duplicates = data.duplicated(subset=['row','col']) == False\n",
    "data_s = data[mask1_duplicates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second mask: Drop extrem individual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop individual holdings where percent_tna is larger than 100 or smaller than 0 -> mask2\n",
    "mask2_individual = (data_s['data'] < 150) & (data_s['data'] > -50)\n",
    "data_s = data_s[mask2_individual]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_row = data_s['row']\n",
    "sparse_col = data_s['col']\n",
    "sparse_data = data_s['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fund/date combinations: 738,860\n",
      "Number of securities: 2,382,968\n"
     ]
    }
   ],
   "source": [
    "sparse_matrix = sparse.csr_matrix((sparse_data, (sparse_row, sparse_col)))\n",
    "print('Number of fund/date combinations: {:,}'.format(sparse_matrix.shape[0]))\n",
    "print('Number of securities: {:,}'.format(sparse_matrix.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third mask: drop extrem portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "716429"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Drop portfolios with total percent_tna > 150 or < 0\n",
    "row_sums = np.array(sparse_matrix.sum(1)).flatten()\n",
    "mask3_portfolios = (row_sums < 150) & (row_sums > 0)\n",
    "np.sum(mask3_portfolios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fund/date combinations after these filters: 716,429\n"
     ]
    }
   ],
   "source": [
    "sparse_matrix = sparse_matrix[mask3_portfolios]\n",
    "print('Number of fund/date combinations after these filters: {:,}'.format(sparse_matrix.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sparse info df with Fund_portno and date for every row of the sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_port_id(num):\n",
    "    start1 = int(np.floor(num / 100_000))\n",
    "    start2 = num - start1 * 100_000\n",
    "    return(start1,start2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.08 s, sys: 152 ms, total: 3.24 s\n",
      "Wall time: 3.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "port_no = []\n",
    "date = []\n",
    "keys = list(port_no_map.keys())\n",
    "\n",
    "for port_IDs in keys:\n",
    "    left_temp, right_temp = split_port_id(port_IDs)\n",
    "    port_no.append(left_temp)\n",
    "    date.append(right_temp)\n",
    "    \n",
    "date = pd.to_timedelta(date, unit='D') + pd.Timestamp('1960-1-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_info = pd.DataFrame(data={'port_no':port_no, 'date':date})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_info = sparse_info[mask3_portfolios]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate table identifying securities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_map = pd.DataFrame.from_dict(stock_map.items())\n",
    "stock_map['crsp_company_key'] = stock_map.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if dimensions of sparse matrix and fund/date and securities info match "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(716429, 2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(716429, 2382968)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse matrix containing holdings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/interim/sparse_matrix'\n",
    "sparse.save_npz(path, sparse_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fund/date info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/interim/sparse_info.feather'\n",
    "feather.write_dataframe(sparse_info,path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Securities info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/interim/stock_map.feather'\n",
    "feather.write_dataframe(stock_map,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
