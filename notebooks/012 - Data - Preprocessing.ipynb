{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preprocess-Large-Holdings-File\" data-toc-modified-id=\"Preprocess-Large-Holdings-File-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preprocess Large Holdings File</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-statements\" data-toc-modified-id=\"Import-statements-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Import statements</a></span></li><li><span><a href=\"#Load-File\" data-toc-modified-id=\"Load-File-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Load File</a></span></li><li><span><a href=\"#Parse-complete-file-to-get-all-unique-fund/date-combinations-and-stocks\" data-toc-modified-id=\"Parse-complete-file-to-get-all-unique-fund/date-combinations-and-stocks-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Parse complete file to get all unique fund/date combinations and stocks</a></span></li><li><span><a href=\"#Parse-complete-file-to-generate-data-for-sparse-matrix\" data-toc-modified-id=\"Parse-complete-file-to-generate-data-for-sparse-matrix-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Parse complete file to generate data for sparse matrix</a></span></li><li><span><a href=\"#Delete-duplicates\" data-toc-modified-id=\"Delete-duplicates-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Delete duplicates</a></span></li><li><span><a href=\"#Check-if-holdings-data-makes-sense\" data-toc-modified-id=\"Check-if-holdings-data-makes-sense-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Check if holdings data makes sense</a></span></li><li><span><a href=\"#Change-fund-info-and-security-info-dfs-for-future-use\" data-toc-modified-id=\"Change-fund-info-and-security-info-dfs-for-future-use-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Change fund info and security info dfs for future use</a></span></li><li><span><a href=\"#Create-sparse-matrix\" data-toc-modified-id=\"Create-sparse-matrix-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Create sparse matrix</a></span></li><li><span><a href=\"#Save-data\" data-toc-modified-id=\"Save-data-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Save data</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Large Holdings File\n",
    "\n",
    "#### Converting the raw 50+ GB sas file with the holdings complezte data into a sparse python matrix which can be loaded into memory and more important which can be handled more efficiently by different alogorithms. \n",
    "#### The logic behind this process is as follows:\n",
    "\n",
    "Loading data and transforming it into csv file to work with\n",
    "\n",
    "1. 50+ GB holdings.sas7bdat file containing all the holdings data downloaded directly from wrds using ftp client\n",
    "2. Converted into csv using sas7bdat_to_csv utility (Link)\n",
    "\n",
    "Two step process to transform file into sparse matrix\n",
    "Challenge is to convert from row describing one holding to rows describing the holdings of one fund at one point in time. Aslo it is crucial to keep track of which row of the sparse matrix is which fund at wjich date and which colums are which securities.\n",
    "\n",
    "3. Open file in python \n",
    "4. Parse through file to make two lists. One with all fund/date combinations (using the comination as an ID) and one with all securities.\n",
    "5. Generate sparse matrix with the dimensions \"number of fund/date combinations\" x \"numer of securities\"\n",
    "6. Parse through large csv file again and fill the percentage_tna (percentage of the fund held in that particular security) number into the right spot of the sparse matrix as determined by two maps based on all fund/date combinations and securities\n",
    "7. Save final sparse matrix and tables containing information about which row is which fund/date and which column is which security."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "Parsing through csv file could be significantly sped up using something like: https://stackoverflow.com/questions/17444679/reading-a-huge-csv-file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import feather\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "path = '../data/raw/holdings.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse complete file to get all unique fund/date combinations and stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "chunksize = 10 ** 7\n",
    "unit = chunksize / 184_578_843\n",
    "\n",
    "reader = pd.read_csv(path,\n",
    "                     usecols = ['crsp_portno','report_dt',\n",
    "                                'crsp_company_key','security_name','cusip'],\n",
    "                     dtype = {'crsp_portno': np.int64,\n",
    "                              'report_dt': np.int64,\n",
    "                              'crsp_company_key': np.int64,\n",
    "                              'security_name': str,\n",
    "                              'cusip': str},\n",
    "                     low_memory=False,\n",
    "                     chunksize=chunksize)\n",
    "\n",
    "dfList_1 = []\n",
    "dfList_2 = []\n",
    "\n",
    "for i, chunk in enumerate(reader):\n",
    "    temp_df_1 = chunk.loc[:,['crsp_portno','report_dt']].drop_duplicates()\n",
    "    temp_df_2 = chunk.loc[:,['crsp_company_key','security_name','cusip']].drop_duplicates()\n",
    "    dfList_1.append(temp_df_1)\n",
    "    dfList_2.append(temp_df_2)\n",
    "\n",
    "    print(\"{:6.2f}%\".format(((i+1) * unit * 100)))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.concat(dfList_1,sort=False)\n",
    "df_2 = pd.concat(dfList_2,sort=False)\n",
    "\n",
    "df_1 = df_1.drop_duplicates()\n",
    "df_2 = df_2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a unique ID from the portno and the date of a fund/date combination\n",
    "df_1 = df_1.assign(port_id = ((df_1['crsp_portno'] * 1000000 + df_1['report_dt'])))\n",
    "df_1 = df_1.rename(columns = {'report_dt':'report_dt_int'})\n",
    "\n",
    "df_1 = df_1.assign(report_dt = pd.to_timedelta(df_1['report_dt_int'], unit='D') + pd.Timestamp('1960-1-1'))\n",
    "\n",
    "df_1 = df_1.reset_index(drop = True)\n",
    "df_1 = (df_1\n",
    "        .assign(row = df_1.index)\n",
    "        .set_index('port_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df_2.reset_index(drop = True)\n",
    "df_2 = (df_2\n",
    "        .assign(col = df_2.index)\n",
    "        .set_index('crsp_company_key'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse complete file to generate data for sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "chunksize = 10 ** 7\n",
    "unit = chunksize / 184_578_843\n",
    "\n",
    "reader = pd.read_csv(path,\n",
    "                     usecols = ['crsp_portno','report_dt','crsp_company_key','percent_tna'],\n",
    "                     dtype = {'crsp_portno': np.int64,\n",
    "                              'report_dt': np.int64,\n",
    "                              'crsp_company_key': np.int64,\n",
    "                              'percent_tna':np.float64},\n",
    "                     low_memory=False,\n",
    "                     chunksize=chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO pd.merge seems to be faster in this case than df.join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dfList = []\n",
    "\n",
    "df_1_temp = df_1.loc[:,['row']]\n",
    "df_2_temp = df_2.loc[:,['col']]\n",
    "\n",
    "for i, chunk in enumerate(reader):\n",
    "    temp_df = chunk.dropna()\n",
    "    temp_df = temp_df.assign(port_id = ((temp_df['crsp_portno'] * 1000000 + temp_df['report_dt'])))\n",
    "    temp_df.set_index('port_id',inplace=True)\n",
    "    temp_df = temp_df.join(df_1_temp, how='left')\n",
    "    temp_df.set_index('crsp_company_key',inplace=True)\n",
    "    temp_df = temp_df.join(df_2_temp, how='left')\n",
    "    temp_df = temp_df[['percent_tna','row','col']]\n",
    "    dfList.append(temp_df)\n",
    "\n",
    "    print(\"{:6.2f}%\".format(((i+1) * unit * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sparse = pd.concat(dfList,sort=False)\n",
    "df_sparse.reset_index(drop=True,inplace=True)\n",
    "print(df_sparse.shape)\n",
    "df_sparse.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete duplicates\n",
    "All other filters will be applied later but this one has to be done before sparse matrix is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_mask = df_sparse.duplicated(['col','row'],keep='last')\n",
    "df_sparse = df_sparse[~duplicates_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if holdings data makes sense "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(df_sparse,df_1[['report_dt','row']],how='left',on='row')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = pd.to_datetime('2016-09-30')\n",
    "\n",
    "sum_col = (merged_data\n",
    "           .query('report_dt == @date')\n",
    "           .groupby(by = ['col'])\n",
    "           .sum()\n",
    "           .sort_values('percent_tna',ascending = False))\n",
    "\n",
    "sum_col.join(df_2.set_index('col'),how='left').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems to make sense. Interestingly Alphabet appears twice. TODO check if two share classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change fund info and security info dfs for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_1[['crsp_portno','report_dt','row']].assign(port_id = df_1.index)\n",
    "df_1.set_index('row',inplace=True)\n",
    "df_1.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df_2.assign(crsp_company_key = df_2.index)\n",
    "df_2.set_index('col',inplace=True)\n",
    "df_2.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix = sparse.csr_matrix((df_sparse['percent_tna'].values, (df_sparse['row'].values, df_sparse['col'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dimensions match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print('Number of fund/date combinations:        {:12,d}'.format(sparse_matrix.shape[0]))\n",
    "print('Number of unique securities:             {:12,d}'.format(sparse_matrix.shape[1]))\n",
    "print('Number of non-zero values in sparse matrix:       {:12,d}'.format(sparse_matrix.getnnz()))\n",
    "print()\n",
    "print('Number of rows in fund info df:          {:12,d}'.format(df_1.shape[0]))\n",
    "print('Number of rows in fund info df:          {:12,d}'.format(df_2.shape[0]))\n",
    "print()\n",
    "match_test = (sparse_matrix.shape[0] == df_1.shape[0]) & (sparse_matrix.shape[1] == df_2.shape[0])\n",
    "print('Everything matches:                              {}'.format(match_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse matrix containing holdings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "path = '../data/interim/holdings'\n",
    "sparse.save_npz(path, sparse_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fund/date info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "path = '../data/interim/row_info.feather'\n",
    "feather.write_dataframe(df_1,path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Securities info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "path = '../data/interim/col_info.feather'\n",
    "feather.write_dataframe(df_2,path)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "MT",
   "language": "python",
   "name": "mt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
